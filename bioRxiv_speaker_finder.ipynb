{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find seminar speakers on bioRxiv\n",
    "This tool extracts first and last authors who have published bioRxiv preprints or Pubmed manuscripts relevant to an inputted subject area. You can use it to find researchers outside of your network to invite them as a speaker or cite their work.\n",
    "\n",
    "This notebook consists of 6 parts, all of which are optional. Fill in the variables as specified & run each cell (Shift+Enter or Play button). Each section prints a list of authors sorted in order of # of relevant manuscripts. More detailed instructions are provided in a markdown cell before each part.\n",
    "\n",
    "**Parts**\n",
    "1. Search bioRxiv. User inputs: keyword, trainee selection\n",
    "2. Search Pubmed. User inputs: same as above, plus email\n",
    "3. Fill in the blanks using ORCID. User inputs: API client ID & key\n",
    "4. Gender API. User inputs: API key\n",
    "5. Ethnicity API. User inputs: API key\n",
    "6. Print to CSV\n",
    "\n",
    "**Important caveats for using the gender & ethnicity APIs**\n",
    "1. The Gender-API predicts gender as a binary based on first name. Gender is not a binary and names do not equal identity.\n",
    "2. The NamSor API predicts a single ethnicity based on full name. The groups it uses are overly broad (only 4 groups: Asian, Hispanic/Latinx, Black/Non-Latinx, White/Non-Latinx), it does not account for multiple ethnicities, and names do not equal ethnicity.\n",
    "3. Remember that these APIs can only provide guesses to help start a search for more diverse speakers. They cannot tell you the gender or ethnicity of any author.\n",
    "\n",
    "If you use this tool, please cite the original paper which created the Rxivist API:  \n",
    "*Abdill RJ, Blekhman R. \"Tracking the popularity and outcomes of all bioRxiv preprints.\" eLife (2019). doi: 10.7554/eLife.45133.*  \n",
    "[Full Rxivist API documentation](https://rxivist.org/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "!pip install biopython\n",
    "from Bio import Entrez\n",
    "import urllib.request\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, fill in the keywords you'd like to search for and whether you are searching for trainees or PIs. Be specific, as the bioRxiv API can only return 250 results maximum.\n",
    "\n",
    "For example:  \n",
    "`keywords = 'sharp wave ripple'\n",
    "trainee = True`  \n",
    "returns a list of first authors on papers that include the terms sharp, wave, and ripple in the title or abstract.\n",
    "\n",
    "`keywords = 'GABA'\n",
    "trainee = False`  \n",
    "returns a list of last authors on papers that include the term GABA in the title or abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'Put your keyword here'\n",
    "trainee = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: BioRxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all papers with the search term\n",
    "#replace spaces for the URL\n",
    "keywords_url = re.sub('\\s','%20',keywords)\n",
    "api_link = 'https://api.rxivist.org/v1/papers?q=' \\\n",
    "    + keywords_url + '&timeframe=alltime&metric=downloads&page_size=250'\n",
    "with urllib.request.urlopen(api_link) as url:\n",
    "    papers = json.loads(url.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build lists of first & last authors with preprint counts\n",
    "authors_dict = dict()\n",
    "\n",
    "for p in papers['results']:\n",
    "    if trainee:\n",
    "        if p['authors'][0]['id'] in authors_dict:\n",
    "            authors_dict[p['authors'][0]['id']] += 1\n",
    "        else:\n",
    "            authors_dict[p['authors'][0]['id']] = 1\n",
    "    else:\n",
    "        if p['authors'][-1]['id'] in authors_dict:\n",
    "            authors_dict[p['authors'][-1]['id']] += 1\n",
    "        else:\n",
    "            authors_dict[p['authors'][-1]['id']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract author information\n",
    "#NB this section calls the API once for each author, so it takes a while\n",
    "authors = list()\n",
    "auth_list = authors_dict.keys()\n",
    "    \n",
    "for author_id in auth_list:\n",
    "    api_link = \"https://api.rxivist.org/v1/authors/\"+str(author_id)\n",
    "    with urllib.request.urlopen(api_link) as url:\n",
    "        author_info = json.loads(url.read().decode())\n",
    "        if not author_info['emails']:\n",
    "            author_info['emails'].append('No email listed')\n",
    "        try:\n",
    "            temp = author_info['emails'][-1]\n",
    "        except:\n",
    "            print(api_link)\n",
    "        authors.append([author_info['name'],author_info['institution'],\n",
    "                        author_info['emails'][-1],author_info['orcid'],\n",
    "                        authors_dict[author_id],'bioRxiv',len(author_info['articles']),\n",
    "                        author_info['ranks'][0]['downloads']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build data frame\n",
    "auth_df = pd.DataFrame(authors, columns=['Last Name','First Name','Institution',\n",
    "                                         'Email','ORCID','Keyword Manuscripts',\n",
    "                                         'Source','Total Preprints','Preprint Downloads'])\n",
    "\n",
    "#print an author list sorted by # relevant preprints\n",
    "auth_df = auth_df.sort_values('Keyword Manuscripts', ascending=False)\n",
    "print(auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Pubmed\n",
    "Expand your search to include published articles from the last 5 years. Similar to the Rxivist API, this does not need a key, but you will need to supply your email so Pubmed can contact you if you are spamming their API too much.\n",
    "\n",
    "Code adapted from [this blog post](https://marcobonzanini.com/2015/01/12/searching-pubmed-with-python/) using the [Entrez API from BioPython](https://biopython.org/DIST/docs/api/Bio.Entrez-module.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = 'Put your email here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an API request for up to 500 papers\n",
    "Entrez.email = email\n",
    "query = '('+keywords+'[Title/Abstract]) AND ((\"2016\"[Date - Publication] : \"3000\"[Date - Publication]))'\n",
    "handle = Entrez.esearch(db='pubmed', sort='relevance', \n",
    "                        retmax='500', retmode='xml', term=query)\n",
    "results = Entrez.read(handle)\n",
    "id_list = ','.join(results['IdList'])\n",
    "\n",
    "#fetch information relevant to each of the queried papers\n",
    "handle = Entrez.efetch(db='pubmed', retmode='xml', id=id_list)\n",
    "papers = Entrez.read(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract author information\n",
    "authors = list()\n",
    "\n",
    "for i, paper in enumerate(papers['PubmedArticle']):\n",
    "    author_info = paper['MedlineCitation']['Article']['AuthorList']\n",
    "    \n",
    "    #pick the first or last author based on user entry\n",
    "    if trainee:\n",
    "        author_info = author_info[0]\n",
    "    else:\n",
    "        author_info = author_info[-1]\n",
    "        \n",
    "    #extract institution, ORCID, & email if available\n",
    "    if not author_info['AffiliationInfo']:\n",
    "        affiliation = ''\n",
    "        email = ''\n",
    "    else:\n",
    "        affiliation = author_info['AffiliationInfo'][0]['Affiliation']\n",
    "        email = re.findall('\\S+@\\S+',affiliation)\n",
    "        if not email:\n",
    "            email = ''\n",
    "        else:\n",
    "            email = email[0][:-1]\n",
    "    if not author_info['Identifier']:\n",
    "        auth_orcid = ''\n",
    "    else:\n",
    "        if not re.findall('orcid.org',author_info['Identifier'][0]):\n",
    "            auth_orcid = 'https://orcid.org/'+author_info['Identifier'][0]\n",
    "        else:\n",
    "            auth_orcid = author_info['Identifier'][0]\n",
    "            \n",
    "    #add to list\n",
    "    authors.append([author_info['LastName'],author_info['ForeName'],affiliation,\n",
    "                    email, auth_orcid, 1, 'Pubmed', NaN, NaN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build data frame\n",
    "auth_df2 = pd.DataFrame(authors, columns=['Last Name','First Name','Institution',\n",
    "                                         'Email','ORCID','Keyword Manuscripts',\n",
    "                                         'Source','Total Preprints','Preprint Downloads'])\n",
    "\n",
    "#if authors have >1 keyword paper, add that to their counts\n",
    "#find duplicates\n",
    "dups = auth_df2.pivot_table(index=['First Name', 'Last Name'], aggfunc='size')\n",
    "#deduplicate\n",
    "auth_df2 = auth_df2[~auth_df2.duplicated(subset=['Last Name','First Name'])]\n",
    "for name in dups[dups>1].axes[0]:\n",
    "    mult_paper_auth = (auth_df2['First Name']==name[0]) & (auth_df2['Last Name']==name[1])\n",
    "    auth_df2.loc[mult_paper_auth,'Keyword Manuscripts'] = dups[name]\n",
    "\n",
    "#print an author list sorted by # relevant preprints\n",
    "auth_df2 = auth_df2.sort_values('Keyword Manuscripts', ascending=False)\n",
    "print(auth_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine bioRxiv & Pubmed lists\n",
    "auf_df = auth_df.append(auth_df2)\n",
    "auth_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fill in the blanks using ORCID\n",
    "Get an [ORCID API client ID & secret](https://support.orcid.org/hc/en-us/articles/360006897174)\n",
    "\n",
    "You can learn more about how to [search for an ORCID](https://members.orcid.org/api/tutorial/search-orcid-registry) and [find info about an author given their ORCID](https://members.orcid.org/api/tutorial/read-orcid-records) in the API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your client ID & key\n",
    "ORCIDAPI_ID = 'YOUR ACCOUNT ID HERE'\n",
    "ORCIDAPI_key = 'YOUR ACCOUNT KEY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a request for a token\n",
    "payload = {'client_id': ORCIDAPI_ID,\n",
    "                   'client_secret': ORCIDAPI_key,\n",
    "                   'scope': '/read-public',\n",
    "                   'grant_type': 'client_credentials'\n",
    "                   }\n",
    "url = 'https://orcid.org/oauth/token'\n",
    "headers = {'Accept': 'application/json'}\n",
    "response = requests.post(url, data=payload, headers=headers, timeout=None)\n",
    "response.raise_for_status()\n",
    "token = response.json()['access_token']\n",
    "\n",
    "# set up headers for searches\n",
    "headers = {'Accept': 'application/vnd.orcid+json',\n",
    "           'Authorization type': 'Bearer',\n",
    "           'Access token': token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of missing ORCIDs\n",
    "#NB this section calls the API once for each author, so it takes a while\n",
    "for index, row in auth_df.iterrows():\n",
    "    if len(row['ORCID'])==0:\n",
    "        given = re.sub(' ','%20',row['First Name'])\n",
    "        family = re.sub(' ','%20',row['Last Name'])\n",
    "#         affil = re.sub(' ','%20',row['Institution'])\n",
    "        \n",
    "        #build search\n",
    "        url = \"https://pub.orcid.org/v3.0/search/?q=\" \\\n",
    "            + \"family-name:\" + family + \"+AND+given-names:\" + given \\\n",
    "            + \"&rows=1\"\n",
    "#             + \"+AND+affiliation-org-name:\" + affil + \"&rows=1\"\n",
    "        auth_id = requests.get(url, headers=headers, timeout=None)\n",
    "        \n",
    "        #get first returned ORCID\n",
    "        if auth_id.json()['result'] is not None:\n",
    "            auth_df.loc[auth_df.index[index], 'ORCID'] = auth_id.json()['result'][0]['orcid-identifier']['uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get email & total works from ORCID\n",
    "for index, row in auth_df.iterrows():\n",
    "    # if we found one, first remove url portion\n",
    "    if len(row['ORCID'])>0:\n",
    "        orcid = re.findall('\\d+-\\S+-\\S+-\\S+', row['ORCID'])\n",
    "\n",
    "        url = \"https://pub.orcid.org/v2.1/\" + ''.join(orcid) + \"/record\"\n",
    "        orcid_request = requests.get(url, headers=headers, timeout=None)\n",
    "        \n",
    "        # replace or add values as needed\n",
    "        if len(row['Email'])>0 and len(orcid_request.json()['person']['emails']['email'])>0:\n",
    "            auth_df.loc[auth_df.index[index], 'Email'] = orcid_request.json()['person']['emails']['email'][0]['email']\n",
    "        if len(orcid_request.json()['activities-summary']['works']['group']) > 0:\n",
    "            auth_df.loc[auth_df.index[index], 'Total Works'] = len(orcid_request.json()['activities-summary']['works']['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new info & print updated list\n",
    "print(auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Gender API\n",
    "If you'd like a list of predicted female authors, run the following 3 blocks.\n",
    "\n",
    "Register for a [Gender-API account](https://gender-api.com/) and add your API code in the code block below.\n",
    "\n",
    "*Note: Free accounts are limited to 500 names per month.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your key\n",
    "genderAPI_key = 'YOUR ACCOUNT KEY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a data frame of only predicted female authors\n",
    "headers={\n",
    "   'X-RapidAPI-Host': 'gender-api.com',\n",
    "   'X-RapidAPI-Key': genderAPI_key\n",
    " }\n",
    "\n",
    "gender_list = list()\n",
    "female_auth_df = pd.DataFrame(columns=['Last Name','First Name','Institution',\n",
    "                                       'Email','ORCID','Keyword Manuscripts',\n",
    "                                       'Source','Total Preprints','Preprint Downloads',\n",
    "                                       'Total Works'])\n",
    "\n",
    "for i, name in enumerate(auth_df['First Name']):\n",
    "    #query API\n",
    "    gender = requests.get('https://gender-api.com/get?name=' + name,\n",
    "                          headers=headers)\n",
    "    gender_list.append(gender.json()['gender'])\n",
    "    #add to df if predicted female\n",
    "    if gender.json()['gender']=='female':\n",
    "        female_auth_df = female_auth_df.append(auth_df.iloc[i])\n",
    "        \n",
    "auth_df['Gender'] = gender_list\n",
    "        \n",
    "print('Authors predicted to be female identifying')\n",
    "print(female_auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Ethnicity API\n",
    "\n",
    "If you'd like to a list of predicted Black & Latinx authors, run the following 3 blocks. This uses the [Namsor-Client](https://pypi.org/project/namsor-client/) package.\n",
    "\n",
    "Register for a [NamSor API account](https://v2.namsor.com/NamSorAPIv2/index.html) and add your API key in the code block below.\n",
    "\n",
    "*Note: Free accounts are limited to 500 names per month.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your key\n",
    "namsor_KEY = \"YOUR ACCOUNT KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install namsor-client\n",
    "from namsorclient import NamsorClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = NamsorClient(namsor_KEY)\n",
    "\n",
    "# find female-identifying authors (same as GenderAPI)\n",
    "# female_auth_df = pd.DataFrame(columns=['Name','Institution','Email',\n",
    "#                                     'Keyword Preprints','Total Preprints','Downloads'])\n",
    "# for i, name in enumerate(auth_df['Name']):\n",
    "#     gender = client.genderFull(name)\n",
    "#     if gender.likely_gender=='female':\n",
    "#         female_auth_df = female_auth_df.append(auth_df.iloc[i])\n",
    "        \n",
    "# print('Authors predicted to be female identifying')\n",
    "# print(female_auth_df)\n",
    "\n",
    "# find minority authors\n",
    "ethnicity_list = list()\n",
    "minority_auth_df = pd.DataFrame(columns=['Last Name','First Name','Institution',\n",
    "                                       'Email','ORCID','Keyword Manuscripts',\n",
    "                                       'Source','Total Preprints','Preprint Downloads',\n",
    "                                       'Total Works'])\n",
    "\n",
    "for i, row in auth_df.iterrows():        \n",
    "    ethnicity = client.usRaceEthnicity(row['First Name'], row['Last Name'])\n",
    "    # options: ['HL', 'A', 'W_NL', 'B_NL']\n",
    "    ethnicity_list.append(ethnicity.race_ethnicity)\n",
    "    if ethnicity.race_ethnicity=='HL' or ethnicity.race_ethnicity=='B_NL':\n",
    "        minority_auth_df = minority_auth_df.append(auth_df.iloc[i])\n",
    "\n",
    "auth_df['Ethnicity'] = ethnicity_list\n",
    "\n",
    "print('Authors predicted to be Black or Latinx')\n",
    "print(minority_auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Save to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = re.sub('\\s','_',keywords) + '_bioRxiv_speaker_finder.csv'\n",
    "auth_df.to_csv(csvfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
