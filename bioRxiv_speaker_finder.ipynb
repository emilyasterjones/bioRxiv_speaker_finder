{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find seminar speakers on bioRxiv\n",
    "This tool extracts first and last authors who have published bioRxiv preprints relevant to an inputted subject area. You can use it to find researchers outside of your network to invite them as a speaker or cite their work.\n",
    "\n",
    "Enter the keywords you'd like to search for and whether you are looking for trainees (first authors) or not in cell 2 (labeled below). Run each cell (Shift+Enter). This notebook will print a list of authors in order by # of relevant preprints and print them to a CSV at the end.  \n",
    "*Note: Be specific, as the API can only return 250 results maximum.*\n",
    "\n",
    "If you'd like to get a list of predicted female or minority authors, follow the instructions for the last 6 code blocks. Each section will print a list of predicted relevant authors, again in order of # of relevant preprints, and append this as a column to the outputted CSV.\n",
    "\n",
    "**Important caveats for using the gender & ethnicity APIs**\n",
    "1. The Gender-API predicts gender as a binary based on first name. Gender is not a binary and names do not equal identity.\n",
    "2. The NamSor API predicts a single ethnicity based on full name. The groups it uses are overly broad (only 4 groups: Asian, Hispanic/Latinx, Black/Non-Latinx, White/Non-Latinx), it does not account for multiple ethnicities, and names do not equal ethnicity.\n",
    "3. Remember that these APIs can only provide guesses to help start a search for more diverse speakers. They cannot tell you the gender or ethnicity of any author.\n",
    "\n",
    "If you use this tool, please cite the original paper which created the Rxivist API:  \n",
    "*Abdill RJ, Blekhman R. \"Tracking the popularity and outcomes of all bioRxiv preprints.\" eLife (2019). doi: 10.7554/eLife.45133.*  \n",
    "[Full API documentation](https://rxivist.org/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, fill in the keywords you'd like to search for and whether you are searching for trainees or PIs.\n",
    "\n",
    "For example:  \n",
    "`keywords = 'sharp wave ripple'\n",
    "trainee = True`  \n",
    "returns a list of first authors on papers that include the terms sharp, wave, and ripple in the title or abstract.\n",
    "\n",
    "`keywords = 'GABA'\n",
    "trainee = False`  \n",
    "returns a list of last authors on papers that include the term GABA in the title or abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'Put your keyword here'\n",
    "trainee = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all papers with the search term\n",
    "#replace spaces for the URL\n",
    "keywords = re.sub('\\s','%20',keywords)\n",
    "api_link = 'https://api.rxivist.org/v1/papers?q=' \\\n",
    "    + keywords + '&timeframe=alltime&metric=downloads&page_size=250'\n",
    "with urllib.request.urlopen(api_link) as url:\n",
    "    papers = json.loads(url.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build lists of first & last authors with preprint counts\n",
    "authors_dict = dict()\n",
    "\n",
    "for p in papers['results']:\n",
    "    if trainee:\n",
    "        if p['authors'][0]['id'] in authors_dict:\n",
    "            authors_dict[p['authors'][0]['id']] += 1\n",
    "        else:\n",
    "            authors_dict[p['authors'][0]['id']] = 1\n",
    "    else:\n",
    "        if p['authors'][-1]['id'] in authors_dict:\n",
    "            authors_dict[p['authors'][-1]['id']] += 1\n",
    "        else:\n",
    "            authors_dict[p['authors'][-1]['id']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract author information\n",
    "#NB this section calls the API once for each author, so it takes a while\n",
    "authors = list()\n",
    "auth_list = authors_dict.keys()\n",
    "    \n",
    "for author_id in auth_list:\n",
    "    api_link = \"https://api.rxivist.org/v1/authors/\"+str(author_id)\n",
    "    with urllib.request.urlopen(api_link) as url:\n",
    "        author_info = json.loads(url.read().decode())\n",
    "        if not author_info['emails']:\n",
    "            author_info['emails'].append('No email listed')\n",
    "        try:\n",
    "            temp = author_info['emails'][-1]\n",
    "        except:\n",
    "            print(api_link)\n",
    "        authors.append([author_info['name'],author_info['institution'],\n",
    "                        author_info['emails'][-1],author_info['orcid']\n",
    "                        authors_dict[author_id],len(author_info['articles']),\n",
    "                        author_info['ranks'][0]['downloads']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Name                                        Institution  \\\n",
      "107          Thad A. Polk   Department of Psychology, University of Michigan   \n",
      "11        Etienne Sibille  Department of Pharmacology and Toxicology, Uni...   \n",
      "40          Michel Loreau  Center for Biodiversity Theory and Modelling, ...   \n",
      "5    Bernardo L. Sabatini  Harvard Medical School, Howard Hughes Medical ...   \n",
      "6             Eric Gouaux                                               OHSU   \n",
      "..                    ...                                                ...   \n",
      "85      Suresh Jesuthasan                  Lee Kong Chian School of Medicine   \n",
      "86      Henning Sprekeler  Modelling of Cognitive Processes, Berlin Insti...   \n",
      "87              Assaf Tal                      Weizmann Institute of Science   \n",
      "88         Veronica Egger  Neurophysiology, Institute of Zoology, Univers...   \n",
      "230         Y. Albert Pan                                      Virginia Tech   \n",
      "\n",
      "                                         Email  Keyword Preprints  \\\n",
      "107                            tpolk@umich.edu                  3   \n",
      "11                 Rammohan.Shukla@utoledo.edu                  3   \n",
      "40          jean-francois.arnoldi@sete.cnrs.fr                  3   \n",
      "5                    bsabatini@hms.harvard.edu                  3   \n",
      "6                             gouauxe@ohsu.edu                  3   \n",
      "..                                         ...                ...   \n",
      "85                         vatsala@ncbs.res.in                  1   \n",
      "86                    h.sprekeler@tu-berlin.de                  1   \n",
      "87                    assaf.tal@weizmann.ac.il                  1   \n",
      "88   veronica.egger@biologie.uni-regensburg.de                  1   \n",
      "230                           yapan@vtc.vt.edu                  1   \n",
      "\n",
      "     Total Preprints  Downloads  \n",
      "107                7       2097  \n",
      "11                10       7459  \n",
      "40                22      12311  \n",
      "5                 18      21338  \n",
      "6                 12      10505  \n",
      "..               ...        ...  \n",
      "85                10       5799  \n",
      "86                 9       7920  \n",
      "87                 1        385  \n",
      "88                 3        601  \n",
      "230                4        848  \n",
      "\n",
      "[231 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#prints an author list sorted by # relevant preprints\n",
    "auth_df = pd.DataFrame(authors, columns=['Name','Institution','Email','ORCID'\n",
    "                                    'Keyword Preprints','Total Preprints','Downloads'])\n",
    "auth_df = auth_df.sort_values('Keyword Preprints', ascending=False)\n",
    "print(auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "Below are code blocks to use the gender & ethnicity APIs to make predictions about the author lists. Use with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender API\n",
    "If you'd like a list of predicted female authors, run the following 3 blocks.\n",
    "\n",
    "Register for a [Gender-API account](https://gender-api.com/) and add your API code in the code block below.\n",
    "\n",
    "*Note: Free accounts are limited to 500 names per month.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your key\n",
    "genderAPI_key = 'YOUR ACCOUNT KEY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first names of all authors\n",
    "first_names = list()\n",
    "for name in auth_df['Name']:\n",
    "    fn = re.findall('^\\S+',name)\n",
    "    fn = fn[0]\n",
    "    #if first name is a letter, try the next name\n",
    "    if len(fn)<3:\n",
    "        fn = re.findall('\\s\\S+\\s',name)\n",
    "        if len(fn)>0:\n",
    "            fn = re.sub('\\s','',fn[0])\n",
    "        else:\n",
    "            fn = ''\n",
    "    first_names.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#build a data frame of only predicted female authors\n",
    "headers={\n",
    "   'X-RapidAPI-Host': 'gender-api.com',\n",
    "   'X-RapidAPI-Key': genderAPI_key\n",
    " }\n",
    "\n",
    "gender_list = list()\n",
    "female_auth_df = pd.DataFrame(columns=['Name','Institution','Email','ORCID'\n",
    "                                    'Keyword Preprints','Total Preprints','Downloads'])\n",
    "\n",
    "for i, name in enumerate(first_names):\n",
    "    #query API\n",
    "    gender = requests.get('https://gender-api.com/get?name=' + name,\n",
    "                          headers=headers)\n",
    "    gender_list.append(gender.json()['gender'])\n",
    "    #add to df if predicted female\n",
    "    if gender.json()['gender']=='female':\n",
    "        female_auth_df = female_auth_df.append(auth_df.iloc[i])\n",
    "        \n",
    "auth_df['Gender'] = gender_list\n",
    "        \n",
    "print('Authors predicted to be female identifying')\n",
    "print(female_auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethnicity API\n",
    "\n",
    "If you'd like to a list of predicted Black & Latinx authors, run the following 3 blocks. This uses the [Namsor-Client](https://pypi.org/project/namsor-client/) package.\n",
    "\n",
    "Register for a [NamSor API account](https://v2.namsor.com/NamSorAPIv2/index.html) and add your API key in the code block below.\n",
    "\n",
    "*Note: Free accounts are limited to 500 names per month.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your key\n",
    "namsor_KEY = \"YOUR ACCOUNT KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install namsor-client\n",
    "from namsorclient import NamsorClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[0 1 2 3 4 7 8 9] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-9b76a0c0b773>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikely_gender\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'male'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#         print(temp.iloc[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3988\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3989\u001b[0m         \"\"\"\n\u001b[1;32m-> 3990\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   3991\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3992\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[0 1 2 3 4 7 8 9] not found in axis'"
     ]
    }
   ],
   "source": [
    "client = NamsorClient(namsor_KEY)\n",
    "\n",
    "# find female-identifying authors (same as GenderAPI)\n",
    "# female_auth_df = pd.DataFrame(columns=['Name','Institution','Email',\n",
    "#                                     'Keyword Preprints','Total Preprints','Downloads'])\n",
    "# for i, name in enumerate(auth_df['Name']):\n",
    "#     gender = client.genderFull(name)\n",
    "#     if gender.likely_gender=='female':\n",
    "#         female_auth_df = female_auth_df.append(auth_df.iloc[i])\n",
    "        \n",
    "# print('Authors predicted to be female identifying')\n",
    "# print(female_auth_df)\n",
    "\n",
    "# find minority authors\n",
    "ethnicity_list = list()\n",
    "minority_auth_df = pd.DataFrame(columns=['Name','Institution','Email','ORCID'\n",
    "                                    'Keyword Preprints','Total Preprints','Downloads'])\n",
    "for i, name in enumerate(auth_df['Name']):\n",
    "    \n",
    "    #separate names into first and last\n",
    "    fn = re.findall('^\\S+',name)\n",
    "    fn = fn[0]\n",
    "    #if first name is a letter, try the next name\n",
    "    if len(fn)<3:\n",
    "        fn = re.findall('\\s\\S+\\s',name)\n",
    "        if len(fn)>0:\n",
    "            fn = re.sub('\\s','',fn[0])\n",
    "        else:\n",
    "            fn = ''\n",
    "    \n",
    "    ln = re.findall('\\S+$',name)\n",
    "    ln = ln[0]\n",
    "        \n",
    "    ethnicity = client.usRaceEthnicity(fn,ln)\n",
    "    # options: ['HL', 'A', 'W_NL', 'B_NL']\n",
    "    ethnicity_list.append(ethnicity.race_ethnicity)\n",
    "    if ethnicity.race_ethnicity=='HL' or ethnicity.race_ethnicity=='B_NL':\n",
    "        minority_auth_df = minority_auth_df.append(auth_df.iloc[i])\n",
    "\n",
    "auth_df['Ethnicity'] = ethnicity_list\n",
    "\n",
    "print('Authors predicted to be Black or Latinx')\n",
    "print(minority_auth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = re.sub('%20','_',keywords) + '_bioRxiv_speaker_finder.csv'\n",
    "auth_df.to_csv(csvfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
